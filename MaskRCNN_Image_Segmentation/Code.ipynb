{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "from samples.idvscript import idvscript\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Path to Ballon trained weights\n",
    "BATTOL_WEIGHTS_PATH = \"/path/to/mask_rcnn_idv_0017.h5\"  # TODO: update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config =idvscript.IDVConfig()\n",
    "BOTTLE_DIR =\"D:\\\\Mask\\\\MaskRCNN_Image_Segmentation\\\\samples\\\\idvscript\\\\dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.9\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                19\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           idv\n",
      "NUM_CLASSES                    7\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                50\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 91\n",
      "Classes: ['BG', '1', '2', '3', '4', '5', '6']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load validation dataset\n",
    "dataset = idvscript.IDVDataset()\n",
    "dataset.load_idv(BOTTLE_DIR, \"val\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 11:51:39.978961 18796 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0913 11:51:39.981954 18796 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0913 11:51:39.985943 18796 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0913 11:51:40.007886 18796 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0913 11:51:40.011874 18796 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0913 11:51:42.332665 18796 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0913 11:51:42.720672 18796 deprecation_wrapper.py:119] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0913 11:51:42.728645 18796 deprecation.py:323] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:399: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0913 11:51:42.734589 18796 deprecation.py:506] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "W0913 11:51:43.059733 18796 deprecation_wrapper.py:119] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:723: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "W0913 11:51:43.065704 18796 deprecation_wrapper.py:119] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:725: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "W0913 11:51:43.183389 18796 deprecation.py:323] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:775: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  D:\\Mask\\MaskRCNN_Image_Segmentation\\mask_rcnn_idv_0017.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "found in frame of :\n",
      "ob:  [ 380  138  700 1021]\n"
     ]
    }
   ],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)\n",
    "weights_path = \"D:\\\\Mask\\\\MaskRCNN_Image_Segmentation\\\\mask_rcnn_idv_0017.h5\"\n",
    "\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mrcnn.config\n",
    "import mrcnn.utils\n",
    "from mrcnn.model import MaskRCNN\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_car_boxes(boxes, class_ids):\n",
    "    car_boxes = []\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        # If the detected object isn't a car / truck, skip it\n",
    "        car_boxes.append(box)\n",
    "\n",
    "    return np.array(car_boxes)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "\n",
    "# Video file or camera to process - set this to 0 to use your webcam instead of a video file\n",
    "VIDEO_SOURCE = \"D:\\\\Mask\\\\MaskRCNN_Image_Segmentation\\\\samples\\\\idvscript\\\\dataset\\\\val\\\\AC (4).jpg\"\n",
    "\n",
    "\n",
    "\n",
    "# Location of parking spaces\n",
    "parked_car_boxes = None\n",
    "\n",
    "# Load the video file we want to run detection on\n",
    "frame = cv2.imread(VIDEO_SOURCE)\n",
    "\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color\n",
    "rgb_image = frame[:, :, ::-1]\n",
    "\n",
    "    # Run the image through the Mask R-CNN model to get results.\n",
    "results = model.detect([rgb_image], verbose=0)\n",
    "\n",
    "    # Mask R-CNN assumes we are running detection on multiple images.\n",
    "    # We only passed in one image to detect, so only grab the first result.\n",
    "r = results[0]\n",
    "\n",
    "print(r['class_ids'])\n",
    "\n",
    "\n",
    "car_boxes = get_car_boxes(r['rois'], r['class_ids'])\n",
    "\n",
    "print(\"found in frame of :\")\n",
    "\n",
    "    # Draw each box on the frame\n",
    "for box in car_boxes:\n",
    "    print(\"ob: \", box)\n",
    "\n",
    "    y1, x1, y2, x2 = box\n",
    "\n",
    "        # Draw the box\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    if r['class_ids'] in [1]:\n",
    "        BC=\"AC\"\n",
    "        cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "    \n",
    "    elif r['class_ids'] in [2]:\n",
    "        BC=\"bottle\"\n",
    "        cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "    \n",
    "    elif r['class_ids'] in [3]:\n",
    "        BC=\"laptop\"\n",
    "        cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "    \n",
    "    elif r['class_ids'] in [4]:\n",
    "        BC=\"mobile\"\n",
    "        cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "    \n",
    "    elif r['class_ids'] in [5]:\n",
    "        BC=\"Shoe\"\n",
    "        cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "    \n",
    "    elif r['class_ids'] in [6]:\n",
    "        BC=\"watch\"\n",
    "        cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)  \n",
    "\n",
    "\n",
    "    # Show the frame of video on the screen\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 12:58:00.932851 14184 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0917 12:58:01.086441 14184 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0917 12:58:01.160423 14184 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0917 12:58:01.287359 14184 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0917 12:58:01.302322 14184 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0917 12:58:03.800221 14184 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0917 12:58:04.283931 14184 deprecation_wrapper.py:119] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0917 12:58:04.291909 14184 deprecation.py:323] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:399: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0917 12:58:04.297849 14184 deprecation.py:506] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "W0917 12:58:04.591098 14184 deprecation_wrapper.py:119] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:723: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "W0917 12:58:04.595091 14184 deprecation_wrapper.py:119] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:725: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "W0917 12:58:04.688804 14184 deprecation.py:323] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:775: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  D:\\Mask\\MaskRCNN_Image_Segmentation\\mask_rcnn_idv_0017.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7b7982c839b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BC'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlineType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class_ids'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m                     \u001b[0mBC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"AC\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBC\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlineType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)\n",
    "weights_path = \"D:\\\\Mask\\\\MaskRCNN_Image_Segmentation\\\\mask_rcnn_idv_0017.h5\"\n",
    "\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mrcnn.config\n",
    "import mrcnn.utils\n",
    "from mrcnn.model import MaskRCNN\n",
    "from pathlib import Path\n",
    "def get_bottle_boxes(boxes, class_ids):\n",
    "    bottle_boxes = []\n",
    "\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        # If the detected object isn't a bottle , skip it\n",
    "        bottle_boxes.append(box)\n",
    "        \n",
    "    return np.array(bottle_boxes)\n",
    "\n",
    "\n",
    "# Location of empty spaces\n",
    "empty_bottle_boxes = None\n",
    "\n",
    "# How many frames of video we've seen in a row with a space open\n",
    "free_space_frames = 0\n",
    "\n",
    "# Video file or camera to process - set this to 0 to use your webcam instead of a video file\n",
    "VIDEO_SOURCE = \"C:\\\\Users\\\\bidhan.roy\\\\Downloads\\\\vid\\\\n4.mp4\"\n",
    "\n",
    "# Load the video file we want to run detection on\n",
    "video_capture = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "\n",
    "\n",
    "# Loop over each frame of video\n",
    "while video_capture.isOpened():\n",
    "    success, frame = video_capture.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color\n",
    "    rgb_image = frame[:, :, ::-1]\n",
    "\n",
    "    # Run the image through the Mask R-CNN model to get results.\n",
    "    results = model.detect([rgb_image], verbose=0)\n",
    "\n",
    "    # Mask R-CNN assumes we are running detection on multiple images.\n",
    "    # We only passed in one image to detect, so only grab the first result.\n",
    "    r = results[0]\n",
    "    \n",
    "\n",
    "    if empty_bottle_boxes is None:\n",
    "        # This is the first frame of video - assume all the bottles detected are in  spaces.\n",
    "        # Save the location of each bottle as a  space box and go to the next frame of video.\n",
    "        empty_bottle_boxes = get_bottle_boxes(r['rois'], r['class_ids'])\n",
    "    else:\n",
    "        # We already know where the spaces are. Check if any are currently unoccupied.\n",
    "\n",
    "        # Get where bottles are currently located in the frame\n",
    "        bottle_boxes = get_bottle_boxes(r['rois'], r['class_ids'])\n",
    "\n",
    "        # See how much those bottles overlap with the known spaces\n",
    "        overlaps = mrcnn.utils.compute_overlaps(empty_bottle_boxes, bottle_boxes)\n",
    "\n",
    "        # Assume no spaces are free until we find one that is free\n",
    "        free_space = False\n",
    "\n",
    "        # Loop through each known space box\n",
    "        for  empting_area, overlap_areas in zip(empty_bottle_boxes, overlaps):\n",
    "\n",
    "            # For this parking space, find the max amount it was covered by any\n",
    "            # bottle that was detected in our image (doesn't really matter which bottle)\n",
    "            max_IoU_overlap = np.max(overlap_areas)\n",
    "\n",
    "            # Get the top-left and bottom-right coordinates of the parking area\n",
    "            y1, x1, y2, x2 =  empting_area\n",
    "\n",
    "            # Check if the  empting space is occupied by seeing if any bottle overlaps\n",
    "            # it by more than 0.15 using IoU\n",
    "            if max_IoU_overlap < 0.15:\n",
    "                #  empting space not occupied! Draw a green box around it\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, 'BC' , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "                if r['class_ids'] in [1]:\n",
    "                    BC=\"AC\"\n",
    "                    cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "\n",
    "                elif r['class_ids'] in [2]:\n",
    "                    BC=\"bottle\"\n",
    "                    cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "\n",
    "                elif r['class_ids'] in [3]:\n",
    "                    BC=\"laptop\"\n",
    "                    cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "\n",
    "                elif r['class_ids'] in [4]:\n",
    "                    BC=\"mobile\"\n",
    "                    cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "\n",
    "                elif r['class_ids'] in [5]:\n",
    "                    BC=\"Shoe\"\n",
    "                    cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)\n",
    "\n",
    "                elif r['class_ids'] in [6]:\n",
    "                    BC=\"watch\"\n",
    "                    cv2.putText(frame, BC , (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0),3,lineType=cv2.LINE_AA)  \n",
    "\n",
    "                # Flag that we have seen at least one open space\n",
    "                \n",
    "                \n",
    "                free_space = True\n",
    "            else:\n",
    "                #  empting space is still occupied - draw a red box around it\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "            # Write the IoU measurement inside the box\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame, f\"{max_IoU_overlap:0.2}\", (x1 + 6, y2 - 6), font, 0.3, (255, 255, 255))\n",
    "\n",
    "        # If at least one space was free, start counting frames\n",
    "        # This is so we don't alert based on one frame of a spot being open.\n",
    "        # This helps prevent the script triggered on one bad detection.\n",
    "        if free_space:\n",
    "            free_space_frames += 1\n",
    "        else:\n",
    "            # If no spots are free, reset the count\n",
    "            free_space_frames = 0\n",
    "\n",
    "        # If a space has been free for several frames, we are pretty sure it is really free!\n",
    "        if free_space_frames > 5:\n",
    "            # Write Empty!! at the top of the screen\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame, f\"Empty!\", (7, 450), font, 3.0, (0, 255, 0), 2, cv2.FILLED)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Show the frame of video on the screen\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up everything when finished\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  D:\\Mask\\MaskRCNN_Image_Segmentation\\mask_rcnn_idv_0017.h5\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-11cd8ed17cab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# Filter the results to only grab the car / truck bounding boxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mcar_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_car_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rois'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"found in frame of :\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)\n",
    "weights_path = \"D:\\\\Mask\\\\MaskRCNN_Image_Segmentation\\\\mask_rcnn_idv_0017.h5\"\n",
    "\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mrcnn.config\n",
    "import mrcnn.utils\n",
    "from mrcnn.model import MaskRCNN\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_car_boxes(boxes, class_ids):\n",
    "    car_boxes = []\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        # If the detected object isn't a car / truck, skip it\n",
    "        car_boxes.append(box)\n",
    "\n",
    "    return np.array(car_boxes)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "\n",
    "# Video file or camera to process - set this to 0 to use your webcam instead of a video file\n",
    "VIDEO_SOURCE = \"D:\\\\Mask\\\\MaskRCNN_Image_Segmentation\\\\samples\\\\idvscript\\\\dataset\\\\val\\\\shoe (10).jpg\"\n",
    "\n",
    "\n",
    "\n",
    "# Location of parking spaces\n",
    "parked_car_boxes = None\n",
    "\n",
    "# Load the video file we want to run detection on\n",
    "frame = cv2.imread(VIDEO_SOURCE)\n",
    "\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color\n",
    "rgb_image = frame[:, :, ::-1]\n",
    "\n",
    "    # Run the image through the Mask R-CNN model to get results.\n",
    "results = model.detect([rgb_image], verbose=0)\n",
    "\n",
    "r = results[0]\n",
    "\n",
    "   \n",
    "\n",
    "    # Filter the results to only grab the car / truck bounding boxes\n",
    "car_boxes = get_car_boxes(r['rois'], r['class'])\n",
    "\n",
    "print(\"found in frame of :\")\n",
    "\n",
    "    # Draw each box on the frame\n",
    "for box in car_boxes:\n",
    "    print(\"ob: \", box)\n",
    "\n",
    "    y1, x1, y2, x2 = box\n",
    "\n",
    "        # Draw the box\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "     \n",
    "\n",
    "    # Show the frame of video on the screen\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up everything when finished\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Mask\\\\MaskRCNN_Image_Segmentation'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\n",
      "image                    shape: (340, 510, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  148.10000  float64\n",
      "image_metas              shape: (1, 19)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n",
      "[3]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-a61b8f1cd59e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mvisualize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_instances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rois'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'masks'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'scores'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Predictions\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\visualize.py\u001b[0m in \u001b[0;36mdisplay_instances\u001b[1;34m(image, boxes, masks, class_ids, class_names, scores, title, figsize, ax, show_mask, show_bbox, colors, captions)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n*** No instances to display *** \\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mclass_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;31m# If no axis is passed, create one and automatically call show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image= cv2.imread(\"D:\\\\Mask\\\\MaskRCNN_Image_Segmentation\\\\samples\\\\idvscript\\\\dataset\\\\val\\\\laptop (7).jpg\")\n",
    "\n",
    "#print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, dataset.image_reference(image_id)))\n",
    "\n",
    "        # Run object detection\n",
    "results = model.detect([image], verbose=1)\n",
    "print(r['class_ids'])\n",
    "if r['class_ids'] in [3]:\n",
    "    BC=\"Lap\"\n",
    "        # Display results\n",
    "ax = get_ax(1)\n",
    "r = results[0]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], BC ,dataset.class_names, r['scores'], ax=ax,title=\"Predictions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0912 15:41:25.463088 18076 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0912 15:41:25.469162 18076 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0912 15:41:25.475289 18076 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0912 15:41:25.511252 18076 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0912 15:41:25.515979 18076 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0912 15:41:30.684115 18076 deprecation_wrapper.py:119] From C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0912 15:41:31.575282 18076 deprecation_wrapper.py:119] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0912 15:41:31.594490 18076 deprecation.py:323] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:399: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0912 15:41:31.609223 18076 deprecation.py:506] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "W0912 15:41:32.272357 18076 deprecation_wrapper.py:119] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:723: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "W0912 15:41:32.288321 18076 deprecation_wrapper.py:119] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:725: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "W0912 15:41:32.506492 18076 deprecation.py:323] From D:\\Mask\\MaskRCNN_Image_Segmentation\\mrcnn\\model.py:775: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  D:\\Mask\\MaskRCNN_Image_Segmentation\\mask_rcnn_idv_0017.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bidhan.roy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)\n",
    "weights_path = \"D:\\\\Mask\\\\MaskRCNN_Image_Segmentation\\\\mask_rcnn_idv_0017.h5\"\n",
    "\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mrcnn.config\n",
    "import mrcnn.utils\n",
    "from mrcnn.model import MaskRCNN\n",
    "from pathlib import Path\n",
    "def get_bottle_boxes(boxes, class_ids):\n",
    "    bottle_boxes = []\n",
    "\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        # If the detected object isn't a bottle , skip it\n",
    "        bottle_boxes.append(box)\n",
    "        \n",
    "    return np.array(bottle_boxes)\n",
    "\n",
    "\n",
    "# Location of empty spaces\n",
    "empty_bottle_boxes = None\n",
    "\n",
    "# How many frames of video we've seen in a row with a space open\n",
    "free_space_frames = 0\n",
    "\n",
    "# Video file or camera to process - set this to 0 to use your webcam instead of a video file\n",
    "VIDEO_SOURCE = \"C:\\\\Users\\\\bidhan.roy\\\\Downloads\\\\vid\\\\n4.mp4\"\n",
    "\n",
    "# Load the video file we want to run detection on\n",
    "video_capture = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "\n",
    "\n",
    "# Loop over each frame of video\n",
    "while video_capture.isOpened():\n",
    "    success, frame = video_capture.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color\n",
    "    rgb_image = frame[:, :, ::-1]\n",
    "\n",
    "    # Run the image through the Mask R-CNN model to get results.\n",
    "    results = model.detect([rgb_image], verbose=0)\n",
    "\n",
    "    # Mask R-CNN assumes we are running detection on multiple images.\n",
    "    # We only passed in one image to detect, so only grab the first result.\n",
    "    r = results[0]\n",
    "    \n",
    "\n",
    "    if empty_bottle_boxes is None:\n",
    "        # This is the first frame of video - assume all the bottles detected are in  spaces.\n",
    "        # Save the location of each bottle as a  space box and go to the next frame of video.\n",
    "        empty_bottle_boxes = get_bottle_boxes(r['rois'], r['class_ids'])\n",
    "    else:\n",
    "        # We already know where the spaces are. Check if any are currently unoccupied.\n",
    "\n",
    "        # Get where bottles are currently located in the frame\n",
    "        bottle_boxes = get_bottle_boxes(r['rois'], r['class_ids'])\n",
    "\n",
    "        # See how much those bottles overlap with the known spaces\n",
    "        overlaps = mrcnn.utils.compute_overlaps(empty_bottle_boxes, bottle_boxes)\n",
    "\n",
    "        # Assume no spaces are free until we find one that is free\n",
    "        free_space = False\n",
    "\n",
    "        # Loop through each known space box\n",
    "        for  empting_area, overlap_areas in zip(empty_bottle_boxes, overlaps):\n",
    "\n",
    "            # For this parking space, find the max amount it was covered by any\n",
    "            # bottle that was detected in our image (doesn't really matter which bottle)\n",
    "            max_IoU_overlap = np.max(overlap_areas)\n",
    "\n",
    "            # Get the top-left and bottom-right coordinates of the parking area\n",
    "            y1, x1, y2, x2 =  empting_area\n",
    "\n",
    "            # Check if the  empting space is occupied by seeing if any bottle overlaps\n",
    "            # it by more than 0.15 using IoU\n",
    "            if max_IoU_overlap < 0.15:\n",
    "                #  empting space not occupied! Draw a green box around it\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                # Flag that we have seen at least one open space\n",
    "                \n",
    "                \n",
    "                free_space = True\n",
    "            else:\n",
    "                #  empting space is still occupied - draw a red box around it\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "            # Write the IoU measurement inside the box\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame, f\"{max_IoU_overlap:0.2}\", (x1 + 6, y2 - 6), font, 0.3, (255, 255, 255))\n",
    "\n",
    "        # If at least one space was free, start counting frames\n",
    "        # This is so we don't alert based on one frame of a spot being open.\n",
    "        # This helps prevent the script triggered on one bad detection.\n",
    "        if free_space:\n",
    "            free_space_frames += 1\n",
    "        else:\n",
    "            # If no spots are free, reset the count\n",
    "            free_space_frames = 0\n",
    "\n",
    "        # If a space has been free for several frames, we are pretty sure it is really free!\n",
    "        if free_space_frames > 5:\n",
    "            # Write Empty!! at the top of the screen\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame, f\"Empty!\", (7, 450), font, 3.0, (0, 255, 0), 2, cv2.FILLED)\n",
    "\n",
    "\n",
    "        # Show the frame of video on the screen\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up everything when finished\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
